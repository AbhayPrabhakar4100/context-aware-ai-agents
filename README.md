# rag-persona-benchmark

## ğŸ§  Overview
This project explores how **Retrieval-Augmented Generation (RAG)** can be leveraged to create and evaluate AI personas.  
By grounding large language model outputs in persona-specific knowledge, we assess how effectively AI agents can mimic roles such as **journalists** or **researchers** while maintaining factual accuracy and stylistic consistency.

---

## ğŸ¯ Goals
- **Persona Simulation** â†’ Generate responses that reflect the behavior and tone of different personas  
- **Knowledge Grounding** â†’ Use RAG to ensure outputs are tied to persona-relevant context  
- **Comparative Evaluation** â†’ Measure how journalist and researcher personas differ in style, accuracy, and depth  

---

## ğŸ“‚ Project Layout
- **/context_files/** â†’ Persona-specific documents used for grounding  
- **/notebooks/** â†’ Jupyter notebooks for experimentation and analysis  
- **/scripts/** â†’ Core pipeline for loading data, embedding, retrieval, and persona evaluation  
- **/results/** â†’ Saved outputs, comparisons, and evaluation metrics  

---

## ğŸ”§ Key Components
- **Retrieval-Augmented Generation (RAG)** â†’ Ensures responses are context-driven and grounded in facts  
- **Persona Definition** â†’ Context files define traits of journalist vs researcher roles  
- **Evaluation Metrics** â†’ Style adherence, factual correctness, and content quality  

---

## ğŸ“Š Example Use Case
1. Define persona context (e.g., *journalist knowledge base*)  
2. Ask a question: *â€œSummarize todayâ€™s economic report.â€*  
3. System retrieves relevant passages + generates answer  
   - **Journalist Persona** â†’ Concise, news-style summary  
   - **Researcher Persona** â†’ Analytical, detailed breakdown  

---

## ğŸš€ Future Extensions
- Add more personas (e.g., policy analyst, teacher, critic)  
- Develop automated persona evaluation metrics  
- Integrate with multi-agent simulations for collaborative tasks  

---
